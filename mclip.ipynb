{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b64f559",
   "metadata": {},
   "source": [
    "# Load in clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569f79d",
   "metadata": {},
   "source": [
    "# Load in multi ling clip and ocr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "# ff75d234-434a-4430-8eee-48a6bea519f8\n",
    "# 0d7fc7c9-f599-11e9-b6ed-001999480be2\n",
    "# ffad79cd-e301-11e6-b8fb-001999480be2\n",
    "ocr_path = \"CLIP\\\\filtered_texts\\\\ffad79cd-e301-11e6-b8fb-001999480be2.xml\"\n",
    "model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n",
    "\n",
    "from pero_ocr.core.layout import PageLayout\n",
    "page_layout = PageLayout(file=ocr_path)\n",
    "texts = []\n",
    "for region in page_layout.regions:\n",
    "    if hasattr(region, 'polygon'):\n",
    "        for line in region.lines:\n",
    "            texts.append(line.transcription)\n",
    "print(texts)\n",
    "\n",
    "# # Load Model & Tokenizer\n",
    "mult_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_embeddings = mult_model.forward(texts, tokenizer)\n",
    "print(text_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2cbb6",
   "metadata": {},
   "source": [
    "# Load in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"CLIP\\\\cropped_images\\\\ffad79cd-e301-11e6-b8fb-001999480be2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "image_features_norm = F.normalize(image_features, dim=-1)\n",
    "text_features_norm = F.normalize(text_embeddings, dim=-1)\n",
    "similarities = torch.matmul(image_features_norm, text_features_norm.T)[0].cpu().tolist()\n",
    "for i,similarity in enumerate(similarities):\n",
    "    print(f\"{similarity}: {texts[i]}\")\n",
    "# print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
