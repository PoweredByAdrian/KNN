{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b75e2f",
   "metadata": {},
   "source": [
    "# CLIP-Based Image-Text Matching Pipeline\n",
    "\n",
    "This notebook demonstrates a complete pipeline for processing, analyzing and matching images with relevant text descriptions using the CLIP neural network. The pipeline includes:\n",
    "\n",
    "1. **Data Acquisition**: Downloading JSON, image, and XML files from a Label Studio project\n",
    "2. **Data Analysis**: Counting and analyzing label distribution in the dataset\n",
    "3. **Data Filtering**: Selecting images and text with specific labels\n",
    "4. **Image Processing**: Cropping images based on annotations\n",
    "5. **Text Processing**: Filtering text regions to remove captions\n",
    "6. **CLIP Processing**: Using CLIP to find semantic matches between images and text\n",
    "\n",
    "Each section below implements one step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af975fa1",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "## 1. Dataset Requirements: OCR XML Files\n",
    "\n",
    "- **OCR XML files** must be present in the `texts/` directory.\n",
    "- Each XML file should correspond to an image in your dataset.\n",
    "- The directory structure should look like:\n",
    "    ```\n",
    "    /texts/\n",
    "            <uuid1>.xml\n",
    "            <uuid2>.xml\n",
    "            ...\n",
    "    ```\n",
    "\n",
    "## 2. Script Requirements\n",
    "\n",
    "The following Python scripts must be available in the working directory:\n",
    "\n",
    "- `download.py`\n",
    "- `count_json.py`\n",
    "- `filter_by_label.py`\n",
    "- `trim_images.py`\n",
    "- `find_label_description.py`\n",
    "- `filter_picture_descriptions.py`\n",
    "- `process_descriptions.py` (requires `cut_text.py`)\n",
    "\n",
    "> **Note:**  \n",
    "> - `process_descriptions.py` depends on `cut_text.py`, so both must be present.\n",
    "- All scripts should be accessible from the notebook's working directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7d81b",
   "metadata": {},
   "source": [
    "Loads necessary modules and configures authentication for Label Studio API access.\n",
    "\n",
    "- Sets up a global debug flag to control logging verbosity\n",
    "- Loads an authentication token from a configuration file\n",
    "- Configures the logging system for a notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b667d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "✓ Directory `downloaded_images` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `texts` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `splitted_jsons` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `filtered_jsons` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `filtered_images` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `filtered_texts` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `filtered_texts_no_desc` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `cropped_images` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `output_context` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Directory `output_jsons` ready"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import torch  # For CLIP model\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# Global configuration\n",
    "DEBUG = False\n",
    "\n",
    "# Define directories for the complete pipeline\n",
    "DIRS = {\n",
    "    # Source directories\n",
    "    \"images_dir\": \"downloaded_images\",  # Where downloaded images will go\n",
    "    \"texts_dir\": \"texts\",               # Where XML files are stored \n",
    "    \"jsons_dir\": \"splitted_jsons\",      # Where split JSON files will go\n",
    "    \n",
    "    # Filtered directories\n",
    "    \"filtered_jsons_dir\": \"filtered_jsons\",      # JSON files with target labels\n",
    "    \"filtered_images_dir\": \"filtered_images\",    # Images with target labels\n",
    "    \"filtered_texts_dir\": \"filtered_texts\",      # XML files with target labels\n",
    "    \n",
    "    # Processed directories\n",
    "    \"filtered_texts_no_desc_dir\": \"filtered_texts_no_desc\",  # XML files without picture descriptions\n",
    "    \"cropped_images_dir\": \"cropped_images\",      # Images cropped to annotation boundaries\n",
    "    \n",
    "    # Output directory\n",
    "    \"output_dir\": \"output_context\",               # Where final output images will go\n",
    "    \"output_jsons_dir\": \"output_jsons\"\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_name, dir_path in DIRS.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    display(Markdown(f\"✓ Directory `{dir_path}` ready\"))\n",
    "\n",
    "# Load token from a config file (not in version control)\n",
    "try:\n",
    "    config_path = Path.home() / \".config\" / \"label_studio_config.json\"\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "        label_studio_token = config.get(\"token\")\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"## ⚠️ Config file not found\"))\n",
    "    display(Markdown(f\"Create a file at {config_path} with the content: `{{'token': 'your_token_here'}}`\"))\n",
    "    label_studio_token = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba297fe3",
   "metadata": {},
   "source": [
    "Downloads the full JSON export from Label Studio containing all task annotations.\n",
    "\n",
    "- Uses the API token for authentication\n",
    "- Saves the export as \"label_studio_export.json\"\n",
    "- Displays the number of tasks downloaded\n",
    "- This export file contains annotation data for all images in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the download module\n",
    "import download\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "download.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Download the export.json file\n",
    "result = download.download_export_json(\n",
    "    token=label_studio_token,\n",
    "    output_file=\"label_studio_export.json\"\n",
    ")\n",
    "\n",
    "if \"error\" in result:\n",
    "    display(Markdown(f\"## ❌ Error\\n{result['error']}\"))\n",
    "else:\n",
    "    labels = result[\"labels\"]\n",
    "    display(Markdown(f\"## Export Successful\\n- Source: {result['source']}\\n- Tasks: {len(labels)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24df7a6",
   "metadata": {},
   "source": [
    "Downloads all annotated images from the Label Studio project:\n",
    "\n",
    "- Only downloads images that have a corresponding XML file in the texts directory\n",
    "- Skips images that already exist locally\n",
    "- Uses the improved progress bar interface with tqdm.notebook\n",
    "- Shows detailed statistics about the download process\n",
    "\n",
    "This step ensures we have all the necessary image data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb679869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run download function with custom directories\n",
    "if label_studio_token:\n",
    "    result = download.run_download(\n",
    "        show_progress=True,\n",
    "        token=label_studio_token,\n",
    "        texts_dir=DIRS[\"texts_dir\"],\n",
    "        images_dir=DIRS[\"images_dir\"]\n",
    "    )\n",
    "    \n",
    "    # Display results as markdown\n",
    "    if \"error\" in result:\n",
    "        display(Markdown(f\"## ❌ Error\\n{result['error']}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"\"\"\n",
    "        ## Download Results\n",
    "        - Total tasks: {result['total_tasks']}\n",
    "        - Downloaded: {result['downloaded']}\n",
    "        - Skipped (already exist): {result['skipped_exists']}\n",
    "        - Skipped (no XML): {result['skipped_no_xml']}\n",
    "        - Failed: {result['failed']}\n",
    "        \"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"## ❌ Cannot proceed without token\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159f25b",
   "metadata": {},
   "source": [
    "Divides the master export file into individual JSON files, one per task:\n",
    "\n",
    "- Extracts each task from the export.json file\n",
    "- Creates a separate JSON file named with the UUID of the task\n",
    "- Verifies that the images and XML files match by comparing UUIDs\n",
    "- Uses the improved progress tracking with tqdm.notebook\n",
    "\n",
    "This step prepares the data for efficient parallel processing in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the export.json into individual JSON files\n",
    "if label_studio_token:\n",
    "    split_result = download.run_split_json(\n",
    "        show_progress=True,\n",
    "        labels_file=\"label_studio_export.json\",\n",
    "        jsons_dir=DIRS[\"jsons_dir\"]\n",
    "    )\n",
    "    \n",
    "    # Display results as markdown\n",
    "    if \"error\" in split_result:\n",
    "        display(Markdown(f\"## ❌ Error\\n{split_result['error']}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"\"\"\n",
    "        ## JSON Split Results\n",
    "        - Total tasks: {split_result['total_tasks']}\n",
    "        - JSON files created: {split_result['json_created']}\n",
    "        - Skipped (no UUID): {split_result['skipped_no_uuid']}\n",
    "        - Failed writes: {split_result['failed_writes']}\n",
    "        - Processing time: {split_result['elapsed_time']:.2f} seconds\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Also run directory comparison to verify we have matching files\n",
    "        compare_result = download.run_compare(\n",
    "            texts_dir=DIRS[\"texts_dir\"],\n",
    "            images_dir=DIRS[\"images_dir\"]\n",
    "        )\n",
    "        if compare_result[\"match\"]:\n",
    "            display(Markdown(\"## ✅ Images and XMLs match!\"))\n",
    "        else:\n",
    "            display(Markdown(f\"\"\"\n",
    "            ## ⚠️ Mismatch between images and XMLs\n",
    "            - Files in both directories: {compare_result['matching_count']}\n",
    "            - Files only in texts directory: {compare_result['texts_only_count']}\n",
    "            - Files only in images directory: {compare_result['images_only_count']}\n",
    "            \"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"## ❌ Cannot proceed without token\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe543c",
   "metadata": {},
   "source": [
    "Analyzes the distribution of rectangle labels in the dataset:\n",
    "\n",
    "- Counts occurrences of each rectangle label across all JSON files\n",
    "- Displays results as a sorted DataFrame for easy analysis\n",
    "- Provides summary statistics about the dataset composition\n",
    "- Uses the improved progress bar implementation with tqdm.notebook\n",
    "\n",
    "This analysis helps identify which labels are most common and can inform filtering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import count_json\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "count_json.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Run the count_labels function with default directory\n",
    "result = count_json.run_count_labels(jsons_dir=DIRS[\"jsons_dir\"], print_table=False)  # Don't print tables in notebook\n",
    "\n",
    "# Display results as a DataFrame\n",
    "label_df = pd.DataFrame(\n",
    "    list(result[\"label_counts\"].items()), \n",
    "    columns=[\"Label\", \"Count\"]\n",
    ").sort_values(\"Count\", ascending=False)\n",
    "\n",
    "display(Markdown(\"## Rectangle Label Counts\"))\n",
    "display(label_df)\n",
    "\n",
    "# Show summary stats\n",
    "display(Markdown(f\"\"\"\n",
    "## Summary Statistics\n",
    "- Total files processed: {result[\"total_files\"]}\n",
    "- Files with rectangle labels: {result[\"processed_files\"] - result[\"no_labels_files\"]}\n",
    "- Files without rectangle labels: {result[\"no_labels_files\"]}\n",
    "- Files with errors: {result[\"error_files\"]}\n",
    "- Total label instances: {result[\"total_labels\"]}\n",
    "- Unique labels found: {result[\"unique_labels\"]}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f123dde",
   "metadata": {},
   "source": [
    "Filters JSON, image, and XML files to keep only those with the target label:\n",
    "\n",
    "- Identifies all JSONs containing the label \"Obrázek\" (Picture)\n",
    "- Copies matching JSONs to the filtered_jsons directory\n",
    "- Copies corresponding images to the filtered_images directory\n",
    "- Copies corresponding XML files to the filtered_texts directory\n",
    "- Features multiple progress bars with the improved tqdm implementation\n",
    "- Updates the directory dictionary with new filtered paths\n",
    "\n",
    "This filtering step ensures we focus only on tasks with images/illustrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the enhanced filtering module\n",
    "import filter_by_label\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "filter_by_label.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Run the filter process with our directory structure for all three file types\n",
    "result = filter_by_label.run_filter_by_label(\n",
    "    jsons_dir=DIRS[\"jsons_dir\"],                # Source JSONs  \n",
    "    images_dir=DIRS[\"images_dir\"],              # Source images\n",
    "    texts_dir=DIRS[\"texts_dir\"],                # Source XML texts\n",
    "    filtered_jsons_dir=\"filtered_jsons\",        # Output filtered JSONs\n",
    "    filtered_images_dir=\"filtered_images\",      # Output filtered images\n",
    "    filtered_texts_dir=\"filtered_texts\",        # Output filtered XMLs\n",
    "    label=\"Obrázek\",                           # Filter by this label\n",
    "    copy_files=True,                            # Copy instead of move\n",
    "    case_sensitive=False                       # Case-insensitive matching\n",
    ")\n",
    "# Display results as markdown\n",
    "display(Markdown(f\"\"\"\n",
    "## Filter Results for Label: '{result[\"label_filtered\"]}'\n",
    "\n",
    "### JSON Files:\n",
    "- With label: {result[\"json_matches\"]}\n",
    "- Without label: {result[\"json_non_matches\"]}\n",
    "- **Total processed: {result[\"json_matches\"] + result[\"json_non_matches\"]}**\n",
    "\n",
    "### Image Files:\n",
    "- Matching filtered JSONs: {result[\"image_matches\"]}\n",
    "- Not matching: {result[\"image_non_matches\"]}\n",
    "- **Total processed: {result[\"image_matches\"] + result[\"image_non_matches\"]}**\n",
    "\n",
    "### Text Files (XML):\n",
    "- Matching filtered JSONs: {result[\"text_matches\"]}\n",
    "- Not matching: {result[\"text_non_matches\"]}\n",
    "- **Total processed: {result[\"text_matches\"] + result[\"text_non_matches\"]}**\n",
    "\n",
    "### Final Dataset:\n",
    "- Total matching triplets (JSON+image+XML): {result[\"total_matching_pairs\"]}\n",
    "- Files were {result[\"file_operation\"].lower()} to filtered directories\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb6505",
   "metadata": {},
   "source": [
    "Processes the filtered images to crop them to their annotated regions:\n",
    "\n",
    "- For each JSON file, finds rectangles with the \"Obrázek\" label\n",
    "- Extracts the coordinates of these rectangles\n",
    "- Crops the corresponding image to these boundaries\n",
    "- Saves the cropped images to a new directory\n",
    "- Uses the enhanced progress bar with tqdm.notebook\n",
    "- Updates the directory dictionary with the cropped images path\n",
    "\n",
    "Cropping lets us focus only on the annotated image content and removes unnecessary background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69823a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the image cropping module\n",
    "import trim_images\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "trim_images.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Run the cropping process with our directory structure\n",
    "result = trim_images.run_crop_images(\n",
    "    jsons_dir=DIRS[\"filtered_jsons_dir\"],      # Use the filtered JSONs\n",
    "    images_dir=DIRS[\"filtered_images_dir\"],    # Use the filtered images \n",
    "    output_dir=\"cropped_images\",               # Where cropped images will go\n",
    "    target_label=\"Obrázek\",                    # Label to look for\n",
    "    show_progress=True                         # Show progress updates\n",
    ")\n",
    "\n",
    "# Display results as markdown\n",
    "if \"error\" in result:\n",
    "    display(Markdown(f\"## ❌ Error\\n{result['error']}\"))\n",
    "else:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ## Image Cropping Results\n",
    "    \n",
    "    - **Processed Files:** {result['files_processed']} of {result['total_files_found']} JSON files\n",
    "    - **Files with Errors:** {result['files_with_errors']}\n",
    "    - **Crops Created:** {result['crops_created']} images\n",
    "    - **Label Used:** \"{result['target_label']}\"\n",
    "    - **Processing Time:** {result['elapsed_time']:.2f} seconds\n",
    "    \n",
    "    All cropped images were saved to `{result['output_dir']}`\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699eaf7",
   "metadata": {},
   "source": [
    "Analyzes the filtered JSONs to identify images that have text descriptions:\n",
    "\n",
    "- Looks for files with the \"Popis v textu\" (Text Description) label\n",
    "- Also checks for co-occurrence of \"Obrázek\" and \"Popis v textu\" labels\n",
    "- Provides detailed statistics about label distribution\n",
    "- Shows a sample of files that contain text descriptions\n",
    "- Uses the enhanced progress tracking with tqdm.notebook\n",
    "\n",
    "This information helps us understand how many images have associated text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the description finding module\n",
    "import find_label_description\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "find_label_description.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Run the analysis function with the filtered JSONs directory\n",
    "result = find_label_description.run_find_descriptions(\n",
    "    jsons_dir=DIRS[\"filtered_jsons_dir\"],\n",
    "    pair_to_check=[\"Obrázek\", \"Popis v textu\"], \n",
    "    print_table=False  # Don't print tables in notebook\n",
    ")\n",
    "\n",
    "# Display label counts as a DataFrame\n",
    "label_df = pd.DataFrame(\n",
    "    list(result[\"label_counts\"].items()), \n",
    "    columns=[\"Label\", \"Count\"]\n",
    ").sort_values(\"Count\", ascending=False)\n",
    "\n",
    "display(Markdown(\"## Rectangle Label Counts\"))\n",
    "display(label_df)\n",
    "\n",
    "# Show summary stats\n",
    "display(Markdown(f\"\"\"\n",
    "## Description Analysis Results\n",
    "- Total files processed: {result[\"total_files\"]}\n",
    "- Files with rectangle labels: {result[\"processed_files\"] - result[\"no_labels_files\"]}\n",
    "- Files without rectangle labels: {result[\"no_labels_files\"]}\n",
    "- Files with errors: {result[\"error_files\"]}\n",
    "- **Files with \"Popis v textu\" label: {result[\"description_count\"]}**\n",
    "- Total label instances: {result[\"total_labels\"]}\n",
    "- Unique labels found: {result[\"unique_labels\"]}\n",
    "\"\"\"))\n",
    "\n",
    "# Show pair co-occurrence if requested\n",
    "if \"pair_checked\" in result:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ## Label Co-occurrence\n",
    "    Files containing BOTH \"{result[\"pair_checked\"][0]}\" AND \"{result[\"pair_checked\"][1]}\": **{result[\"pair_count\"]}**\n",
    "    \"\"\"))\n",
    "\n",
    "# Show sample of files with descriptions\n",
    "if result[\"files_with_description\"]:\n",
    "    sample_files = result[\"files_with_description\"][:10]  # Show first 10\n",
    "    sample_list = \"\\n\".join([f\"- {file}\" for file in sample_files])\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "    ## Files with Descriptions\n",
    "    Sample of files containing \"Popis v textu\" label ({len(result[\"files_with_description\"])} total):\n",
    "    {sample_list}\n",
    "    {'...(and more)' if len(result[\"files_with_description\"]) > 10 else ''}\n",
    "    \"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"## No files with descriptions were found\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31744f",
   "metadata": {},
   "source": [
    "Removes text regions that are explicitly labeled as picture descriptions:\n",
    "\n",
    "- Matches \"Popis u obrázku\" (Picture Description) regions in JSON files\n",
    "- Identifies the corresponding text regions in XML files using IoU matching\n",
    "- Removes those text regions from the XML files\n",
    "- Creates new filtered XML files without description text\n",
    "- Uses enhanced progress tracking with tqdm.notebook\n",
    "- Updates the directory dictionary with the filtered text path\n",
    "\n",
    "This step ensures CLIP doesn't match images with their existing captions in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the picture description filtering module\n",
    "import filter_picture_descriptions\n",
    "\n",
    "# Setup logging for notebook environment\n",
    "filter_picture_descriptions.setup_logging(debug_mode=DEBUG, use_notebook=True)\n",
    "\n",
    "# Run the filtering process with our directory structure\n",
    "result = filter_picture_descriptions.run_filter_descriptions(\n",
    "    json_dir=DIRS[\"filtered_jsons_dir\"],             # JSON files from filtered directory \n",
    "    xml_dir=DIRS[\"filtered_texts_dir\"],              # XML files from filtered directory\n",
    "    output_dir=\"filtered_texts_no_desc\",             # Output directory for filtered XMLs\n",
    "    iou_threshold=0.00005,                           # IoU threshold for matching\n",
    "    show_progress=True                               # Show progress updates (ensure this is True)\n",
    ")\n",
    "\n",
    "# Display results as markdown\n",
    "if \"error\" in result:\n",
    "    display(Markdown(f\"## ❌ Error\\n{result['error']}\"))\n",
    "else:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ## Picture Description Filtering Results\n",
    "    \n",
    "    - **Processed Files:** {result['processed_files']} file pairs\n",
    "    - **Files with Matches:** {result['files_with_matches']}\n",
    "    - **Files Copied Without Filtering:** {result['copied_without_filtering']}\n",
    "    \n",
    "    ### Matching Statistics:\n",
    "    - Total picture description regions: {result['total_json_regions']}\n",
    "    - Matched regions: {result['total_matches']}\n",
    "    {f\"- Match percentage: {result['match_percentage']:.2f}%\" if 'match_percentage' in result else \"\"}\n",
    "    - Text regions removed from XML: {result['regions_removed']}\n",
    "    \n",
    "    ### Processing Details:\n",
    "    - IoU threshold used: {result['iou_threshold']}\n",
    "    - Processing time: {result['elapsed_time']:.2f} seconds\n",
    "    \n",
    "    Filtered XML files are available in: `{result['output_dir']}`\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cfa83",
   "metadata": {},
   "source": [
    "Uses OpenAI's CLIP model to find semantic matches between images and text:\n",
    "\n",
    "- Loads the CLIP ViT-B/32 model (handles both images and text)\n",
    "- For each cropped image, computes CLIP embeddings\n",
    "- For each text region in the XML files, computes CLIP embeddings\n",
    "- Calculates cosine similarity between image and text embeddings\n",
    "- Identifies text that semantically matches each image\n",
    "- Creates visual context images showing the image with matched text\n",
    "- Uses nested progress bars with improved tqdm implementation\n",
    "- Displays a sample result with the matched image and text\n",
    "\n",
    "This is the core of the pipeline that creates the final image-text matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de489b2",
   "metadata": {},
   "source": [
    "# Filter JSONs by Text Description IDs Test only\n",
    "\n",
    "This section creates a subset of filtered JSONs containing only those that have text descriptions.\n",
    "It reads IDs from a list file and copies the matching JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72685c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Current Directory Structure ---\n",
      "Working directory: c:\\Users\\Bahno\\Desktop\\skola\\8sem\\KNN\\new\\KNN\\CLIP\n",
      "Directory 'filtered_jsons' exists with 3949 files\n",
      "Sample files: ['001fb740-cc2b-11ea-b34d-5ef3fc9bb22f.json', '003d9392-3e40-11e1-bdd3-005056a60003.json', '0045b280-d504-11e3-893a-0030487be43a.json']\n",
      "Directory 'filtered_description_jsons' exists with 0 files\n",
      "\n",
      "--- Running Filter Operation ---\n",
      "Error reading IDs file: [Errno 2] No such file or directory: 'test.txt'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Filter Results for JSONs with Text Descriptions\n",
       "\n",
       "- Total IDs in list: 0\n",
       "- Files found in source directory: 0\n",
       "- Files missing from source: 0\n",
       "- Files successfully copied: 0\n",
       "- Errors during copy: 0\n",
       "\n",
       "Filtered JSONs with text descriptions are available in: `filtered_description_jsons`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Filter Operation ---\n",
      "Output directory 'filtered_description_jsons' contains 0 files\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "⚠️ No files were copied. Keeping original filtered_jsons_dir."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Keeping original filtered_jsons_dir: filtered_jsons\n"
     ]
    }
   ],
   "source": [
    "DIRS[\"filtered_jsons_dir\"] = \"filtered_jsons\"\n",
    "\n",
    "def filter_jsons_by_id_list(\n",
    "    ids_file: str,\n",
    "    source_dir: str,\n",
    "    output_dir: str,\n",
    "    show_progress: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Filter JSON files by IDs listed in a text file.\n",
    "    \n",
    "    Args:\n",
    "        ids_file: Path to text file containing IDs (one per line, with or without .json extension)\n",
    "        source_dir: Directory containing source JSON files\n",
    "        output_dir: Directory where filtered JSONs will be saved\n",
    "        show_progress: Whether to show a progress bar\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import glob\n",
    "    import logging\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(\"filter_jsons\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read IDs from file\n",
    "    try:\n",
    "        with open(ids_file, 'r', encoding='utf-8') as f:\n",
    "            ids = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Read {len(ids)} IDs from {ids_file}\")\n",
    "        # Debug: Show first 5 IDs \n",
    "        print(f\"Sample IDs from file: {ids[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading IDs file: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    # Get source files\n",
    "    source_files = glob.glob(os.path.join(source_dir, \"*.json\"))\n",
    "    \n",
    "    # Debug: Print source directory and file count\n",
    "    print(f\"Source directory: {source_dir}\")\n",
    "    print(f\"Source files found: {len(source_files)}\")\n",
    "    # Debug: Show first few source files\n",
    "    print(f\"Sample source files: {[os.path.basename(f) for f in source_files[:5]]}\")\n",
    "    \n",
    "    # Prepare for progress bar\n",
    "    if show_progress:\n",
    "        try:\n",
    "            file_iterator = notebook_tqdm(ids, desc=\"Filtering JSONs\", unit=\"file\")\n",
    "        except:\n",
    "            file_iterator = ids\n",
    "    else:\n",
    "        file_iterator = ids\n",
    "    \n",
    "    # Track statistics\n",
    "    stats = {\n",
    "        \"total_ids\": len(ids),\n",
    "        \"found_files\": 0,\n",
    "        \"missing_files\": 0,\n",
    "        \"copied_files\": 0,\n",
    "        \"errors\": 0\n",
    "    }\n",
    "    \n",
    "    # First check if the directory even exists\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"❌ ERROR: Source directory '{source_dir}' does not exist!\")\n",
    "        stats[\"error\"] = f\"Source directory '{source_dir}' not found\"\n",
    "        return stats\n",
    "    \n",
    "    # Store missed filenames for debugging\n",
    "    missed_files = []\n",
    "    \n",
    "    # Process each ID\n",
    "    for filename in file_iterator:\n",
    "        # Debug: Print current ID being processed\n",
    "        if stats[\"missing_files\"] < 5 or stats[\"found_files\"] < 5:\n",
    "            print(f\"Processing ID: {filename}\")\n",
    "        \n",
    "        # Try to find the source file - test multiple path formats\n",
    "        source_paths_to_try = [\n",
    "            os.path.join(source_dir, filename),                    # Original filename\n",
    "            os.path.join(source_dir, filename.replace('.json', '')),  # Without .json\n",
    "            os.path.join(source_dir, f\"{filename}.json\")           # With .json\n",
    "        ]\n",
    "        \n",
    "        source_path = None\n",
    "        for path_to_try in source_paths_to_try:\n",
    "            if os.path.exists(path_to_try):\n",
    "                source_path = path_to_try\n",
    "                if stats[\"found_files\"] < 5:\n",
    "                    print(f\"✓ Found file: {source_path}\")\n",
    "                break\n",
    "        \n",
    "        if not source_path:\n",
    "            stats[\"missing_files\"] += 1\n",
    "            if stats[\"missing_files\"] <= 10:\n",
    "                print(f\"❌ Missing file: {filename}\")\n",
    "                missed_files.append(filename)\n",
    "            continue\n",
    "                \n",
    "        stats[\"found_files\"] += 1\n",
    "        \n",
    "        try:\n",
    "            # Copy file to destination (always use the same filename format as the source)\n",
    "            dest_path = os.path.join(output_dir, os.path.basename(source_path))\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            stats[\"copied_files\"] += 1\n",
    "            if stats[\"copied_files\"] <= 5:\n",
    "                print(f\"✓ Copied: {source_path} → {dest_path}\")\n",
    "        except Exception as e:\n",
    "            stats[\"errors\"] += 1\n",
    "            print(f\"❌ Error copying {source_path}: {e}\")\n",
    "    \n",
    "    # Print summary of missed files\n",
    "    if missed_files:\n",
    "        print(f\"\\nFirst 10 missing files: {missed_files[:10]}\")\n",
    "        \n",
    "        # Try to check if files without .json extension exist\n",
    "        source_files_no_ext = [os.path.splitext(os.path.basename(f))[0] for f in source_files]\n",
    "        found_without_ext = [f for f in missed_files if f.replace('.json', '') in source_files_no_ext]\n",
    "        if found_without_ext:\n",
    "            print(f\"\\n✓ Found {len(found_without_ext)} files when ignoring extension!\")\n",
    "            print(f\"Sample: {found_without_ext[:5]}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Create a new directory for filtered JSONs with descriptions\n",
    "filtered_description_jsons_dir = \"filtered_description_jsons\"\n",
    "os.makedirs(filtered_description_jsons_dir, exist_ok=True)\n",
    "\n",
    "# Debug: Print directory contents before running filter\n",
    "print(\"\\n--- Current Directory Structure ---\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "dirs_to_check = ['filtered_jsons', 'filtered_description_jsons']\n",
    "for d in dirs_to_check:\n",
    "    if os.path.exists(d):\n",
    "        files = os.listdir(d)\n",
    "        print(f\"Directory '{d}' exists with {len(files)} files\")\n",
    "        if files:\n",
    "            print(f\"Sample files: {files[:3]}\")\n",
    "    else:\n",
    "        print(f\"Directory '{d}' does not exist\")\n",
    "\n",
    "# Run the filtering function with the modified function\n",
    "print(\"\\n--- Running Filter Operation ---\")\n",
    "results = filter_jsons_by_id_list(\n",
    "    ids_file=\"test.txt\",             # TEST\n",
    "    \n",
    "    #ids_file=\"popis_v_textu_files.txt\",             # File containing IDs\n",
    "    source_dir=DIRS[\"filtered_jsons_dir\"],          # Source directory (filtered JSONs)\n",
    "    output_dir=filtered_description_jsons_dir,      # Output directory\n",
    "    show_progress=True                              # Show progress bar\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display(Markdown(f\"\"\"\n",
    "## Filter Results for JSONs with Text Descriptions\n",
    "\n",
    "- Total IDs in list: {results.get(\"total_ids\", 0)}\n",
    "- Files found in source directory: {results.get(\"found_files\", 0)}\n",
    "- Files missing from source: {results.get(\"missing_files\", 0)}\n",
    "- Files successfully copied: {results.get(\"copied_files\", 0)}\n",
    "- Errors during copy: {results.get(\"errors\", 0)}\n",
    "\n",
    "Filtered JSONs with text descriptions are available in: `{filtered_description_jsons_dir}`\n",
    "\"\"\"))\n",
    "\n",
    "# Debug: Check output directory contents after filtering\n",
    "print(\"\\n--- After Filter Operation ---\")\n",
    "if os.path.exists(filtered_description_jsons_dir):\n",
    "    files = os.listdir(filtered_description_jsons_dir)\n",
    "    print(f\"Output directory '{filtered_description_jsons_dir}' contains {len(files)} files\")\n",
    "    if files:\n",
    "        print(f\"Sample files: {files[:3]}\")\n",
    "\n",
    "# Only update the directory reference if files were found and copied\n",
    "if results.get(\"copied_files\", 0) > 0:\n",
    "    DIRS[\"filtered_description_jsons_dir\"] = filtered_description_jsons_dir\n",
    "    DIRS[\"filtered_jsons_dir\"] = filtered_description_jsons_dir\n",
    "    print(f\"✓ Updated DIRS dictionary with new filtered path: {filtered_description_jsons_dir}\")\n",
    "else:\n",
    "    # Keep using the original directory\n",
    "    display(Markdown(\"⚠️ No files were copied. Keeping original filtered_jsons_dir.\"))\n",
    "    print(f\"⚠️ Keeping original filtered_jsons_dir: {DIRS['filtered_jsons_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf090121",
   "metadata": {},
   "source": [
    "# Run this before running CLIP if not testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a32ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIRS[\"filtered_jsons_dir\"] = \"filtered_jsons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: INFO logging enabled - use debug=True for more details\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✓ Found 3949 files in `filtered_jsons`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Found 7715 files in `cropped_images`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Found 3949 files in `filtered_texts_no_desc`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "⚠️ Found 50 files in `output_context`. Clearing directory..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Cleared directory `output_context`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "⚠️ Found 100 files in `output_jsons`. Clearing directory..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✓ Cleared directory `output_jsons`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ⚙️ Running CLIP model - this may take several minutes..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device: cpu\n",
      "INFO: Loading CLIP model: ViT-B/32\n",
      "INFO: Model loaded in 3.13 seconds\n",
      "INFO: Starting processing with ViT-B/32 model\n",
      "INFO: Scanning for JSONs with 'Obrázek' label...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c12bad344b430a98fa560374bf4019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking JSONs for Obrázek label:   0%|          | 0/3949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Limiting to 100 IDs out of 3949 total\n",
      "INFO: Found 100 IDs to process\n",
      "INFO: Config: threshold=0.25, model=ViT-B/32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4198560309c34f18ba859508ce3f20cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing IDs:   0%|          | 0/100 [00:00<?, ?ID/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m display(Markdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## ⚙️ Running CLIP model - this may take several minutes...\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Run the processing function with our directory structure\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_descriptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_process_descriptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_jsons_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Use filtered JSONs\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcropped_images_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Use cropped images \u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_texts_no_desc_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use filtered texts without descriptions\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Where output files will go\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_images_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Original images directory\u001b[39;49;00m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Threshold for text matching\u001b[39;49;00m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lines_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Include 3 lines above/below matches\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# Process 100 IDs\u001b[39;49;00m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"M-CLIP\",\u001b[39;49;00m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-B/32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# CLIP model to use\u001b[39;49;00m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\u001b[38;5;66;43;03m# Use all matching blocks above threshold (not just best)\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# Find top 3 matching blocks for each image\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# Use tqdm.notebook for progress bars\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# Disable verbose output for cleaner logs\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_jsons_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIRS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_jsons_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Where output JSONs will go\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Display results as markdown\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\Desktop\\skola\\8sem\\KNN\\new\\KNN\\CLIP\\process_descriptions.py:900\u001b[0m, in \u001b[0;36mrun_process_descriptions\u001b[1;34m(json_dir, images_dir, texts_dir, original_images_dir, output_dir, output_jsons_dir, similarity_threshold, max_lines_context, max_image_suffix, max_ids, model_name, process_all, specific_id, best_only, top_k, verbose, show_progress)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, id_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(id_iterator):\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;66;03m# Removed duplicate logging - let the progress bar show this info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;66;03m# Only keep debug level logging for individual ID processing\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids_to_process)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 900\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_images_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_images_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_jsons_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_jsons_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_lines_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_lines_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_image_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_image_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add the top_k parameter\u001b[39;49;00m\n\u001b[0;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbest_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\Desktop\\skola\\8sem\\KNN\\new\\KNN\\CLIP\\process_descriptions.py:501\u001b[0m, in \u001b[0;36mprocess_id\u001b[1;34m(id_value, model, preprocess, images_dir, texts_dir, original_images_dir, output_dir, output_jsons_dir, similarity_threshold, max_lines_context, max_image_suffix, top_k, device, best_only, verbose, text_model, tokenizer)\u001b[0m\n\u001b[0;32m    494\u001b[0m json_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m][image_id_with_suffix] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_path,\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_similarities\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m    498\u001b[0m }\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# Find top matching blocks for this image\u001b[39;00m\n\u001b[1;32m--> 501\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfind_top_matching_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxml_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\Desktop\\skola\\8sem\\KNN\\new\\KNN\\CLIP\\process_descriptions.py:366\u001b[0m, in \u001b[0;36mfind_top_matching_blocks\u001b[1;34m(image_path, xml_path, model, preprocess, top_k, device, text_model, tokenizer)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# 2. Preprocess image\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 366\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m     image_input \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\Desktop\\skola\\8sem\\KNN\\new\\KNN\\venv\\lib\\site-packages\\PIL\\Image.py:3505\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3502\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3505\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import the process descriptions module\n",
    "import glob\n",
    "import process_descriptions\n",
    "import shutil\n",
    "\n",
    "# Set up logging for notebook environment\n",
    "process_descriptions.setup_logging(debug_mode=DEBUG, use_notebook=True, log_to_file=False)\n",
    "\n",
    "# Check if required directories have data\n",
    "required_dirs = {\n",
    "    \"filtered_jsons_dir\": DIRS[\"filtered_jsons_dir\"],\n",
    "    \"cropped_images_dir\": DIRS[\"cropped_images_dir\"],\n",
    "    \"filtered_texts_no_desc_dir\": DIRS[\"filtered_texts_no_desc_dir\"]\n",
    "}\n",
    "\n",
    "for name, path in required_dirs.items():\n",
    "    file_count = len(glob.glob(os.path.join(path, \"*\")))\n",
    "    display(Markdown(f\"✓ Found {file_count} files in `{path}`\"))\n",
    "\n",
    "# Clear output directories before processing\n",
    "output_dirs = [DIRS[\"output_dir\"], DIRS[\"output_jsons_dir\"]]\n",
    "for dir_path in output_dirs:\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(dir_path):\n",
    "        # Get a list of all files in the directory\n",
    "        files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "        if files:\n",
    "            # Ask for confirmation before deleting\n",
    "            display(Markdown(f\"⚠️ Found {len(files)} files in `{dir_path}`. Clearing directory...\"))\n",
    "            \n",
    "            # Delete all files in the directory\n",
    "            for file_path in files:\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.unlink(file_path)\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                except Exception as e:\n",
    "                    display(Markdown(f\"❌ Error deleting {file_path}: {e}\"))\n",
    "            \n",
    "            display(Markdown(f\"✓ Cleared directory `{dir_path}`\"))\n",
    "        else:\n",
    "            display(Markdown(f\"✓ Directory `{dir_path}` is already empty\"))\n",
    "\n",
    "# Let's add a notification that this might take a while\n",
    "display(Markdown(\"## ⚙️ Running CLIP model - this may take several minutes...\"))\n",
    "\n",
    "# Run the processing function with our directory structure\n",
    "result = process_descriptions.run_process_descriptions(\n",
    "    json_dir=DIRS[\"filtered_jsons_dir\"],      # Use filtered JSONs\n",
    "    images_dir=DIRS[\"cropped_images_dir\"],    # Use cropped images \n",
    "    texts_dir=DIRS[\"filtered_texts_no_desc_dir\"],  # Use filtered texts without descriptions\n",
    "    output_dir=DIRS[\"output_dir\"],            # Where output files will go\n",
    "    original_images_dir=DIRS[\"images_dir\"],   # Original images directory\n",
    "    similarity_threshold=0.25,                # Threshold for text matching\n",
    "    max_lines_context=3,                      # Include 3 lines above/below matches\n",
    "    max_ids=100,                              # Process 100 IDs\n",
    "    model_name=\"M-CLIP\",\n",
    "    # model_name=\"ViT-B/32\",                    # CLIP model to use\n",
    "    best_only=False,                          # Use all matching blocks above threshold (not just best)\n",
    "    top_k=5,                                  # Find top 3 matching blocks for each image\n",
    "    show_progress=True,                       # Use tqdm.notebook for progress bars\n",
    "    verbose=False,                            # Disable verbose output for cleaner logs\n",
    "    output_jsons_dir=DIRS[\"output_jsons_dir\"] # Where output JSONs will go\n",
    ")\n",
    "\n",
    "# Display results as markdown\n",
    "if \"error\" in result:\n",
    "    display(Markdown(f\"## ❌ Error\\n{result['error']}\"))\n",
    "else:\n",
    "    display(Markdown(f\"\"\"\n",
    "    ## CLIP Text-Image Matching Results\n",
    "    \n",
    "    ### Processing Summary:\n",
    "    - Total IDs processed: {result['summary']['total_ids']}\n",
    "    - Successful matches: {result['summary']['successful_ids']}\n",
    "    - Success rate: {result['summary']['success_rate']:.1f}%\n",
    "    - Total images processed: {result['summary']['total_images_processed']}\n",
    "    - Images below threshold: {result['summary']['images_below_threshold']}\n",
    "    \n",
    "    ### Configuration:\n",
    "    - Model used: {result['config']['model']} on {result['config']['device']}\n",
    "    - Similarity threshold: {result['config']['similarity_threshold']}\n",
    "    - Max lines context: {result['config']['max_lines_context']}\n",
    "    - Top-k matches: 3 (finds top 3 matching text blocks per image)\n",
    "    \n",
    "    ### Performance:\n",
    "    - Total processing time: {result['summary']['elapsed_time']:.2f} seconds\n",
    "    - Average time per ID: {result['summary']['average_time_per_id']:.2f} seconds\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Show a sample image if any were successful\n",
    "    successful_results = [r for r in result['details'] if r.get('success', False)]\n",
    "    if successful_results:\n",
    "        sample = successful_results[0]\n",
    "        display(Markdown(f\"### Sample Result: ID {sample['id']}\"))\n",
    "        sample_path = os.path.join(DIRS[\"output_dir\"], sample['output_file'])\n",
    "        if os.path.exists(sample_path):\n",
    "            display(Image(filename=sample_path, width=800))\n",
    "            display(Markdown(f\"- Context blocks: {sample['context_blocks']}\"))\n",
    "            display(Markdown(f\"- Processing time: {sample['time']:.2f} seconds\"))\n",
    "        else:\n",
    "            display(Markdown(f\"Image file not found: {sample_path}\"))\n",
    "    else:\n",
    "        display(Markdown(\"No successful results to display\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badba3fb",
   "metadata": {},
   "source": [
    "# Intersection Analysis\n",
    "\n",
    "This section analyzes whether the CLIP-matched text regions intersect with manually labeled \"Popis v textu\" (text description) regions.\n",
    "\n",
    "- Compares the output JSONs from CLIP with the filtered JSONs containing annotations\n",
    "- Identifies where context blocks from CLIP match with human-annotated text descriptions\n",
    "- Calculates statistics on overlap/intersection between AI and human annotations\n",
    "- Shows examples of matches where they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a06e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 samples with merged context blocks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca535b3891c4712b248bb6263470328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating visualizations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Bounding Box Comparison Summary (with merged context blocks)\n",
       "\n",
       "### Processing Statistics:\n",
       "- Total samples: 100\n",
       "- Successfully processed: 100\n",
       "- Failed to process: 0\n",
       "\n",
       "### Box Detection:\n",
       "- Total CLIP context boxes: 109\n",
       "- Total manual annotation boxes: 49\n",
       "- Total intersections detected: 23\n",
       "- Overall intersection rate: 21.10%\n",
       "\n",
       "### Sample Intersections:\n",
       "- Samples with at least one intersection: 17\n",
       "- Percentage of samples with intersections: 17.00%\n",
       "\n",
       "### Top Samples by Intersection Count:\n",
       "064aef66-735d-4b34-93b3-770b48c037d3 (3 intersections), 02928146-e6ab-11e5-bc5e-001b21d0d3a4 (2 intersections), 04aa82b9-1cbb-4ae0-b65c-1cbae3c6a9cd (2 intersections), 04c338cf-0061-11e7-9c30-92f789bb8157 (2 intersections), 08abce05-4802-40a4-a48b-5dce1a0bf24e (2 intersections)\n",
       "\n",
       "All visualizations have been saved to the 'bbox_comparisons' directory.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enhanced debugging for CLIP bounding boxes with intersection detection for all examples\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image as PILImage\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    Boxes are in format (x, y, width, height) as fractions.\n",
    "    Returns IoU value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Convert to coordinates format [x1, y1, x2, y2]\n",
    "    box1_coords = [box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3]]\n",
    "    box2_coords = [box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3]]\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    x_left = max(box1_coords[0], box2_coords[0])\n",
    "    y_top = max(box1_coords[1], box2_coords[1])\n",
    "    x_right = min(box1_coords[2], box2_coords[2])\n",
    "    y_bottom = min(box1_coords[3], box2_coords[3])\n",
    "    \n",
    "    # No intersection\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # Calculate union area\n",
    "    box1_area = box1[2] * box1[3]\n",
    "    box2_area = box2[2] * box2[3]\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area if union_area > 0 else 0.0\n",
    "    return iou\n",
    "\n",
    "def check_intersection(box1, box2):\n",
    "    \"\"\"\n",
    "    Check if two boxes intersect.\n",
    "    Boxes are in format (x, y, width, height) as fractions.\n",
    "    Returns True if boxes intersect, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert to coordinates format [x1, y1, x2, y2]\n",
    "    box1_coords = [box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3]]\n",
    "    box2_coords = [box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3]]\n",
    "    \n",
    "    # Check if boxes intersect\n",
    "    if (box1_coords[2] <= box2_coords[0] or  # box1 is to the left of box2\n",
    "        box1_coords[0] >= box2_coords[2] or  # box1 is to the right of box2\n",
    "        box1_coords[3] <= box2_coords[1] or  # box1 is above box2\n",
    "        box1_coords[1] >= box2_coords[3]):   # box1 is below box2\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def process_bounding_box(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Process a bounding box and convert to normalized format.\n",
    "    \n",
    "    Args:\n",
    "        bbox: Bounding box in format [x1, y1, x2, y2] in pixels\n",
    "        img_width: Width of the image\n",
    "        img_height: Height of the image\n",
    "        \n",
    "    Returns:\n",
    "        Tuple (x, y, width, height) as fractions or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure we have numeric values\n",
    "        bbox = [float(coord) for coord in bbox]\n",
    "        \n",
    "        # Convert from pixels [x1, y1, x2, y2] to fractions [x, y, width, height]\n",
    "        x = bbox[0] / img_width\n",
    "        y = bbox[1] / img_height\n",
    "        width = (bbox[2] - bbox[0]) / img_width\n",
    "        height = (bbox[3] - bbox[1]) / img_height\n",
    "        return (x, y, width, height)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def merge_blocks_in_context(block_boxes):\n",
    "    \"\"\"\n",
    "    Merge multiple bounding boxes into a single encompassing box.\n",
    "    \n",
    "    Args:\n",
    "        block_boxes: List of bounding boxes in (x, y, width, height) format\n",
    "        \n",
    "    Returns:\n",
    "        A single bounding box that encompasses all input boxes\n",
    "    \"\"\"\n",
    "    if not block_boxes:\n",
    "        return None\n",
    "    \n",
    "    if len(block_boxes) == 1:\n",
    "        return block_boxes[0]\n",
    "    \n",
    "    # Find the min and max coordinates\n",
    "    min_x = min(box[0] for box in block_boxes)\n",
    "    min_y = min(box[1] for box in block_boxes)\n",
    "    max_x = max(box[0] + box[2] for box in block_boxes)\n",
    "    max_y = max(box[1] + box[3] for box in block_boxes)\n",
    "    \n",
    "    # Create a new bounding box that encompasses all the boxes\n",
    "    merged_box = (min_x, min_y, max_x - min_x, max_y - min_y)\n",
    "    return merged_box\n",
    "\n",
    "def extract_clip_bboxes(output_data, sample_id, img_width, img_height, verbose=False, merge_context=True):\n",
    "    \"\"\"\n",
    "    Extract all clip bounding boxes from the output data, handling different formats.\n",
    "    \n",
    "    Args:\n",
    "        output_data: The loaded JSON data\n",
    "        sample_id: ID of the sample to extract boxes for\n",
    "        img_width: Width of the image\n",
    "        img_height: Height of the image\n",
    "        verbose: Whether to print detailed info\n",
    "        merge_context: Whether to merge context blocks within a main block into one\n",
    "        \n",
    "    Returns:\n",
    "        List of bounding boxes in format [(x, y, width, height), ...]\n",
    "    \"\"\"\n",
    "    clip_bboxes = []\n",
    "    \n",
    "    # Check which format we're dealing with\n",
    "    if 'images' in output_data and sample_id in output_data['images']:\n",
    "        # New format with nested structure\n",
    "        image_data = output_data['images'][sample_id]\n",
    "        if verbose:\n",
    "            print(f\"Found image data for {sample_id} in nested format\")\n",
    "            \n",
    "        if 'blocks' in image_data:\n",
    "            for block_idx, block in enumerate(image_data['blocks']):\n",
    "                # Process main block\n",
    "                if 'bounding_box' in block:\n",
    "                    main_bbox = process_bounding_box(block['bounding_box'], img_width, img_height)\n",
    "                    \n",
    "                    # Process context blocks nested within this block\n",
    "                    context_boxes = []\n",
    "                    if 'context_blocks' in block and block['context_blocks']:\n",
    "                        for ctx_idx, ctx_block in enumerate(block['context_blocks']):\n",
    "                            if 'bounding_box' in ctx_block:\n",
    "                                ctx_bbox = process_bounding_box(ctx_block['bounding_box'], img_width, img_height)\n",
    "                                if ctx_bbox:\n",
    "                                    context_boxes.append(ctx_bbox)\n",
    "                                    if verbose and not merge_context:\n",
    "                                        print(f\"Added context block {block_idx}.{ctx_idx} bounding box\")\n",
    "                    \n",
    "                    # If we have both main block and context blocks and merging is enabled\n",
    "                    if main_bbox and context_boxes and merge_context:\n",
    "                        # Include the main block in the merge if it's valid\n",
    "                        all_boxes = [main_bbox] + context_boxes\n",
    "                        merged_box = merge_blocks_in_context(all_boxes)\n",
    "                        clip_bboxes.append(merged_box)\n",
    "                        if verbose:\n",
    "                            print(f\"Added merged block {block_idx} with {len(context_boxes)} context blocks\")\n",
    "                    else:\n",
    "                        # Add main block if it exists\n",
    "                        if main_bbox:\n",
    "                            clip_bboxes.append(main_bbox)\n",
    "                            if verbose:\n",
    "                                print(f\"Added main block {block_idx} bounding box\")\n",
    "                        \n",
    "                        # Add individual context blocks if merging is disabled\n",
    "                        if not merge_context:\n",
    "                            clip_bboxes.extend(context_boxes)\n",
    "                    \n",
    "                # If no main bounding box but has context blocks\n",
    "                elif 'context_blocks' in block and block['context_blocks']:\n",
    "                    context_boxes = []\n",
    "                    for ctx_idx, ctx_block in enumerate(block['context_blocks']):\n",
    "                        if 'bounding_box' in ctx_block:\n",
    "                            ctx_bbox = process_bounding_box(ctx_block['bounding_box'], img_width, img_height)\n",
    "                            if ctx_bbox:\n",
    "                                context_boxes.append(ctx_bbox)\n",
    "                                if verbose and not merge_context:\n",
    "                                    print(f\"Added context block {block_idx}.{ctx_idx} bounding box\")\n",
    "                    \n",
    "                    # Merge context blocks if enabled and we have any\n",
    "                    if context_boxes and merge_context:\n",
    "                        merged_box = merge_blocks_in_context(context_boxes)\n",
    "                        clip_bboxes.append(merged_box)\n",
    "                        if verbose:\n",
    "                            print(f\"Added merged context blocks for block {block_idx} ({len(context_boxes)} boxes)\")\n",
    "                    elif not merge_context:\n",
    "                        clip_bboxes.extend(context_boxes)\n",
    "    \n",
    "    # Handle the older formats\n",
    "    elif 'context_blocks' in output_data:\n",
    "        if verbose:\n",
    "            print(f\"Using old format with direct context_blocks\")\n",
    "            \n",
    "        # Old format with direct context_blocks\n",
    "        context_boxes = []\n",
    "        for block in output_data['context_blocks']:\n",
    "            if 'bbox' in block:\n",
    "                # Bbox in the format [x1, y1, x2, y2] as fractions\n",
    "                bbox = block['bbox']\n",
    "                \n",
    "                # Convert from [x1, y1, x2, y2] to [x, y, width, height]\n",
    "                x = bbox[0]\n",
    "                y = bbox[1]\n",
    "                width = bbox[2] - bbox[0]\n",
    "                height = bbox[3] - bbox[1]\n",
    "                context_boxes.append((x, y, width, height))\n",
    "                \n",
    "            elif 'bounding_box' in block:\n",
    "                # Bbox in the format [x1, y1, x2, y2] in pixels\n",
    "                bbox = process_bounding_box(block['bounding_box'], img_width, img_height)\n",
    "                if bbox:\n",
    "                    context_boxes.append(bbox)\n",
    "        \n",
    "        # Merge all context blocks if enabled\n",
    "        if context_boxes and merge_context:\n",
    "            merged_box = merge_blocks_in_context(context_boxes)\n",
    "            clip_bboxes.append(merged_box)\n",
    "            if verbose:\n",
    "                print(f\"Merged all {len(context_boxes)} context blocks into one\")\n",
    "        else:\n",
    "            clip_bboxes.extend(context_boxes)\n",
    "    \n",
    "    # Check for alternative structure\n",
    "    elif 'blocks' in output_data:\n",
    "        if verbose:\n",
    "            print(f\"Using direct blocks format\")\n",
    "            \n",
    "        # Direct blocks array\n",
    "        block_boxes = []\n",
    "        for block in output_data['blocks']:\n",
    "            if 'bounding_box' in block:\n",
    "                bbox = process_bounding_box(block['bounding_box'], img_width, img_height)\n",
    "                if bbox:\n",
    "                    block_boxes.append(bbox)\n",
    "        \n",
    "        # Merge all blocks if enabled\n",
    "        if block_boxes and merge_context:\n",
    "            merged_box = merge_blocks_in_context(block_boxes)\n",
    "            clip_bboxes.append(merged_box)\n",
    "            if verbose:\n",
    "                print(f\"Merged all {len(block_boxes)} blocks into one\")\n",
    "        else:\n",
    "            clip_bboxes.extend(block_boxes)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Extracted {len(clip_bboxes)} CLIP bounding boxes\" + \n",
    "              (\" (after merging)\" if merge_context else \"\"))\n",
    "        \n",
    "    return clip_bboxes\n",
    "\n",
    "def visualize_comparison(output_json_dir, filtered_json_dir, output_dir, sample_id=None, verbose=False, merge_context=True):\n",
    "    \"\"\"\n",
    "    Visualize an image with its CLIP context bboxes (red) and manual annotation bboxes (green).\n",
    "    Also highlights intersecting boxes with yellow borders and reports intersection statistics.\n",
    "    \n",
    "    Args:\n",
    "        output_json_dir: Directory containing CLIP output JSONs\n",
    "        filtered_json_dir: Directory containing filtered JSONs with annotations\n",
    "        output_dir: Directory to save visualizations\n",
    "        sample_id: Specific ID to process (if None, processes first available)\n",
    "        verbose: Whether to print detailed debug info\n",
    "        merge_context: Whether to merge context blocks within a main block\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with result information or None if failed\n",
    "    \"\"\"\n",
    "    # Get available IDs\n",
    "    output_jsons = [f for f in os.listdir(output_json_dir) if f.endswith('_results.json')]\n",
    "    \n",
    "    if not output_jsons:\n",
    "        if verbose:\n",
    "            print(f\"❌ No output JSONs with '_results.json' suffix found in directory: {output_json_dir}\")\n",
    "        # Try without suffix as fallback\n",
    "        output_jsons = [f for f in os.listdir(output_json_dir) if f.endswith('.json')]\n",
    "        if not output_jsons:\n",
    "            if verbose:\n",
    "                print(f\"❌ No JSON files found in directory: {output_json_dir}\")\n",
    "            return None\n",
    "    \n",
    "    # If no specific ID is provided, use the first one\n",
    "    if sample_id is None:\n",
    "        sample_json = output_jsons[0]\n",
    "        sample_id = os.path.splitext(sample_json)[0]\n",
    "        if sample_id.endswith('_results'):\n",
    "            sample_id = sample_id[:-8]  # Remove \"_results\" suffix\n",
    "    else:\n",
    "        # Ensure sample_id has no extension\n",
    "        sample_id = os.path.splitext(sample_id)[0]\n",
    "        if sample_id.endswith('_results'):\n",
    "            sample_id = sample_id[:-8]  # Remove \"_results\" suffix\n",
    "    \n",
    "    # Check if both JSONs exist\n",
    "    output_json_path = os.path.join(output_json_dir, f\"{sample_id}_results.json\")\n",
    "    if not os.path.exists(output_json_path):\n",
    "        output_json_path = os.path.join(output_json_dir, f\"{sample_id}.json\")  # Try without suffix\n",
    "        if not os.path.exists(output_json_path):\n",
    "            if verbose:\n",
    "                print(f\"❌ Output JSON not found: {output_json_path}\")\n",
    "            return None\n",
    "    \n",
    "    filtered_json_path = os.path.join(filtered_json_dir, f\"{sample_id}.json\")\n",
    "    if not os.path.exists(filtered_json_path):\n",
    "        if verbose:\n",
    "            print(f\"❌ Filtered JSON not found: {filtered_json_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load the JSONs\n",
    "    try:\n",
    "        with open(output_json_path, 'r') as f:\n",
    "            output_data = json.load(f)\n",
    "        with open(filtered_json_path, 'r') as f:\n",
    "            filtered_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"❌ Error loading JSONs: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Get image path\n",
    "    image_path = os.path.join(DIRS[\"images_dir\"], f\"{sample_id}.jpg\")\n",
    "    if not os.path.exists(image_path):\n",
    "        # Try other extensions\n",
    "        for ext in ['.png', '.jpeg', '.gif']:\n",
    "            alt_path = os.path.join(DIRS[\"images_dir\"], f\"{sample_id}{ext}\")\n",
    "            if os.path.exists(alt_path):\n",
    "                image_path = alt_path\n",
    "                break\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            if verbose:\n",
    "                print(f\"❌ Image not found for ID: {sample_id}\")\n",
    "            return None\n",
    "    \n",
    "    # Load the image\n",
    "    img = PILImage.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Get bounding boxes from CLIP output using the enhanced extraction function with merging\n",
    "    clip_bboxes = extract_clip_bboxes(output_data, sample_id, img_width, img_height, verbose, merge_context)\n",
    "    \n",
    "    # Manual bounding boxes from filtered JSON\n",
    "    manual_bboxes = []\n",
    "    \n",
    "    # Helper function to extract annotations from result\n",
    "    def extract_manual_annotations(result_list):\n",
    "        boxes = []\n",
    "        for result in result_list:\n",
    "            if 'value' in result and 'rectanglelabels' in result['value']:\n",
    "                label = result['value']['rectanglelabels']\n",
    "                if isinstance(label, list) and ('Popis v textu' in label):\n",
    "                    try:\n",
    "                        # Get coordinates (already in percentages)\n",
    "                        x = float(result['value']['x']) / 100.0  # Convert from percentage to fraction\n",
    "                        y = float(result['value']['y']) / 100.0\n",
    "                        width = float(result['value']['width']) / 100.0\n",
    "                        height = float(result['value']['height']) / 100.0\n",
    "                        \n",
    "                        boxes.append((x, y, width, height))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        return boxes\n",
    "    \n",
    "    # Extract boxes from result array (different JSON structures)\n",
    "    if 'result' in filtered_data:\n",
    "        manual_bboxes.extend(extract_manual_annotations(filtered_data['result']))\n",
    "    elif 'annotations' in filtered_data and len(filtered_data['annotations']) > 0:\n",
    "        for annotation in filtered_data['annotations']:\n",
    "            if 'result' in annotation:\n",
    "                manual_bboxes.extend(extract_manual_annotations(annotation['result']))\n",
    "    \n",
    "    # Check for intersections between CLIP and manual boxes\n",
    "    intersections = []\n",
    "    for i, clip_box in enumerate(clip_bboxes):\n",
    "        for j, manual_box in enumerate(manual_bboxes):\n",
    "            if check_intersection(clip_box, manual_box):\n",
    "                iou = calculate_iou(clip_box, manual_box)\n",
    "                intersections.append((i, j, iou))\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(np.array(img))\n",
    "    \n",
    "    # Plot CLIP bboxes in red\n",
    "    for i, bbox in enumerate(clip_bboxes):\n",
    "        x, y, width, height = bbox\n",
    "        # Scale to pixel values\n",
    "        x_px = x * img_width\n",
    "        y_px = y * img_height\n",
    "        width_px = width * img_width\n",
    "        height_px = height * img_height\n",
    "        \n",
    "        # Check if this box intersects with any manual box\n",
    "        is_intersecting = any(intersection[0] == i for intersection in intersections)\n",
    "        \n",
    "        # Use thicker border for intersecting boxes\n",
    "        linewidth = 3 if is_intersecting else 2\n",
    "        # Use different color for intersecting boxes\n",
    "        edgecolor = 'orange' if is_intersecting else 'r'\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x_px, y_px), width_px, height_px,\n",
    "            linewidth=linewidth, edgecolor=edgecolor, facecolor='none',\n",
    "            label='CLIP Context (Intersecting)' if (i == 0 and is_intersecting) else \n",
    "                  ('CLIP Context' if i == 0 else \"\")\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text label inside the box\n",
    "        ax.text(x_px + 5, y_px + 15, f\"C{i+1}\", color=edgecolor, fontweight='bold',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0))\n",
    "    \n",
    "    # Plot manual bboxes in green\n",
    "    for i, bbox in enumerate(manual_bboxes):\n",
    "        x, y, width, height = bbox\n",
    "        # Scale to pixel values\n",
    "        x_px = x * img_width\n",
    "        y_px = y * img_height\n",
    "        width_px = width * img_width\n",
    "        height_px = height * img_height\n",
    "        \n",
    "        # Check if this box intersects with any CLIP box\n",
    "        is_intersecting = any(intersection[1] == i for intersection in intersections)\n",
    "        \n",
    "        # Use thicker border for intersecting boxes\n",
    "        linewidth = 3 if is_intersecting else 2\n",
    "        # Use different color for intersecting boxes\n",
    "        edgecolor = 'orange' if is_intersecting else 'g'\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x_px, y_px), width_px, height_px,\n",
    "            linewidth=linewidth, edgecolor=edgecolor, facecolor='none',\n",
    "            label='Manual Annotation (Intersecting)' if (i == 0 and is_intersecting) else \n",
    "                  ('Manual Annotation' if i == 0 else \"\")\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text label inside the box\n",
    "        ax.text(x_px + 5, y_px + 15, f\"M{i+1}\", color=edgecolor, fontweight='bold',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0))\n",
    "    \n",
    "    # Add legend (only once for each color)\n",
    "    handles = []\n",
    "    if clip_bboxes:\n",
    "        handles.append(patches.Patch(linewidth=2, edgecolor='r', facecolor='none', label='CLIP Context (C#)'))\n",
    "    if manual_bboxes:\n",
    "        handles.append(patches.Patch(linewidth=2, edgecolor='g', facecolor='none', label='Manual Annotation (M#)'))\n",
    "    if intersections:\n",
    "        handles.append(patches.Patch(linewidth=3, edgecolor='orange', facecolor='none', label='Intersection'))\n",
    "    \n",
    "    if handles:\n",
    "        ax.legend(handles=handles, loc='upper right')\n",
    "    \n",
    "    # Set title with intersection stats and merging info\n",
    "    merge_info = \" (merged)\" if merge_context else \"\"\n",
    "    intersection_info = f\" - {len(intersections)} intersections\" if intersections else \"\"\n",
    "    ax.set_title(f\"ID: {sample_id} - CLIP ({len(clip_bboxes)}){merge_info} vs Manual ({len(manual_bboxes)}){intersection_info}\")\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Save the figure\n",
    "    output_path = os.path.join(output_dir, f\"{sample_id}_comparison{'_merged' if merge_context else ''}.jpg\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"output_path\": output_path,\n",
    "        \"clip_boxes\": len(clip_bboxes),\n",
    "        \"manual_boxes\": len(manual_bboxes),\n",
    "        \"intersections\": len(intersections),\n",
    "        \"id\": sample_id,\n",
    "        \"merged\": merge_context\n",
    "    }\n",
    "\n",
    "def run_batch_visualizations(output_json_dir, filtered_json_dir, output_dir, max_samples=None, verbose=False, merge_context=True):\n",
    "    \"\"\"\n",
    "    Run visualizations on multiple samples.\n",
    "    \n",
    "    Args:\n",
    "        output_json_dir: Directory containing CLIP output JSONs\n",
    "        filtered_json_dir: Directory containing filtered JSONs with annotations\n",
    "        output_dir: Directory to save visualizations\n",
    "        max_samples: Maximum number of samples to process (None for all)\n",
    "        verbose: Whether to print detailed logs\n",
    "        merge_context: Whether to merge context blocks within a main block\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all available IDs\n",
    "    output_jsons = [f for f in os.listdir(output_json_dir) if f.endswith('_results.json')]\n",
    "    if not output_jsons:\n",
    "        output_jsons = [f for f in os.listdir(output_json_dir) if f.endswith('.json')]\n",
    "    \n",
    "    sample_ids = []\n",
    "    for filename in output_jsons:\n",
    "        sample_id = os.path.splitext(filename)[0]\n",
    "        if sample_id.endswith('_results'):\n",
    "            sample_id = sample_id[:-8]  # Remove \"_results\" suffix\n",
    "        sample_ids.append(sample_id)\n",
    "    \n",
    "    # Limit sample size if requested\n",
    "    if max_samples is not None and max_samples < len(sample_ids):\n",
    "        sample_ids = sample_ids[:max_samples]\n",
    "    \n",
    "    # Process all samples with progress bar\n",
    "    merge_status = \"merged\" if merge_context else \"individual\"\n",
    "    print(f\"Processing {len(sample_ids)} samples with {merge_status} context blocks...\")\n",
    "    \n",
    "    results = []\n",
    "    total_clip_boxes = 0\n",
    "    total_manual_boxes = 0\n",
    "    total_intersections = 0\n",
    "    successful_samples = 0\n",
    "    \n",
    "    try:\n",
    "        # Use tqdm.notebook for progress bar in Jupyter\n",
    "        sample_iterator = notebook_tqdm(sample_ids, desc=\"Generating visualizations\")\n",
    "    except:\n",
    "        # Fallback to regular iteration if tqdm fails\n",
    "        sample_iterator = sample_ids\n",
    "        \n",
    "    for sample_id in sample_iterator:\n",
    "        result = visualize_comparison(\n",
    "            output_json_dir=output_json_dir,\n",
    "            filtered_json_dir=filtered_json_dir,\n",
    "            output_dir=output_dir,\n",
    "            sample_id=sample_id,\n",
    "            verbose=verbose,\n",
    "            merge_context=merge_context\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            total_clip_boxes += result[\"clip_boxes\"]\n",
    "            total_manual_boxes += result[\"manual_boxes\"]\n",
    "            total_intersections += result[\"intersections\"]\n",
    "            successful_samples += 1\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary = {\n",
    "        \"total_samples\": len(sample_ids),\n",
    "        \"successful_samples\": successful_samples,\n",
    "        \"failed_samples\": len(sample_ids) - successful_samples,\n",
    "        \"total_clip_boxes\": total_clip_boxes,\n",
    "        \"total_manual_boxes\": total_manual_boxes,\n",
    "        \"total_intersections\": total_intersections,\n",
    "        \"intersection_rate\": (total_intersections / total_clip_boxes * 100) if total_clip_boxes > 0 else 0,\n",
    "        \"results\": results,\n",
    "        \"merge_context\": merge_context\n",
    "    }\n",
    "    \n",
    "    # Calculate samples with intersections\n",
    "    samples_with_intersections = len([r for r in results if r[\"intersections\"] > 0])\n",
    "    summary[\"samples_with_intersections\"] = samples_with_intersections\n",
    "    summary[\"intersection_sample_rate\"] = (samples_with_intersections / successful_samples * 100) if successful_samples > 0 else 0\n",
    "    \n",
    "    # Find best samples with most intersections\n",
    "    if results:\n",
    "        # Sort by number of intersections (descending)\n",
    "        best_samples = sorted(results, key=lambda x: x[\"intersections\"], reverse=True)[:5]\n",
    "        summary[\"best_samples\"] = best_samples\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Set output directory for visualizations\n",
    "comparison_dir = \"bbox_comparisons\"\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "# Run batch visualization for all samples with merged context blocks\n",
    "summary = run_batch_visualizations(\n",
    "    output_json_dir=DIRS[\"output_jsons_dir\"],\n",
    "    filtered_json_dir=DIRS[\"filtered_jsons_dir\"],\n",
    "    output_dir=comparison_dir,\n",
    "    max_samples=None,  # Process all available samples\n",
    "    verbose=False,     # Don't print debug info\n",
    "    merge_context=True # Merge context blocks within each main block\n",
    ")\n",
    "\n",
    "# Display only the summary statistics\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "merge_info = \"(with merged context blocks)\" if summary.get(\"merge_context\", False) else \"\"\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Bounding Box Comparison Summary {merge_info}\n",
    "\n",
    "### Processing Statistics:\n",
    "- Total samples: {summary['total_samples']}\n",
    "- Successfully processed: {summary['successful_samples']}\n",
    "- Failed to process: {summary['failed_samples']}\n",
    "\n",
    "### Box Detection:\n",
    "- Total CLIP context boxes: {summary['total_clip_boxes']}\n",
    "- Total manual annotation boxes: {summary['total_manual_boxes']}\n",
    "- Total intersections detected: {summary['total_intersections']}\n",
    "- Overall intersection rate: {summary['intersection_rate']:.2f}%\n",
    "\n",
    "### Sample Intersections:\n",
    "- Samples with at least one intersection: {summary['samples_with_intersections']}\n",
    "- Percentage of samples with intersections: {summary['intersection_sample_rate']:.2f}%\n",
    "\n",
    "### Top Samples by Intersection Count:\n",
    "{', '.join([f\"{s['id']} ({s['intersections']} intersections)\" for s in summary['best_samples'][:5]])}\n",
    "\n",
    "All visualizations have been saved to the 'bbox_comparisons' directory.\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
